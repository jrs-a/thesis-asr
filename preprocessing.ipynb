{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare files\n",
    "\n",
    "Prepare first the list of files in the format `[id path duration(s)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sox\n",
    "with open(\"vad.lst\", \"w\") as fout, open(\"audio.lst\", \"r\") as f:\n",
    "  for n, name in enumerate(f):\n",
    "    name = name.strip()\n",
    "    fout.write(\"{}\\t{}\\t{}\\n\".format(\n",
    "        n, name, sox.file_info.duration(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head vad.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm can be skipped, then no perplexity value wil be in the result file\n",
    "!./flashlight/build/bin/asr/fl_asr_voice_activity_detection_ctc \\\n",
    "   --am=am_transformer_ctc_stride3_letters_300Mparams.bin \\\n",
    "   --tokens=tokens.txt \\\n",
    "   --lexicon=lexicon.txt \\\n",
    "   --lm=lm_common_crawl_small_4gram_prun0-6-15_200kvocab.bin \\\n",
    "   --test=vad.lst \\\n",
    "   --outpath=vad-result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check VAD result\n",
    "\n",
    "Let's have a look at results we have in the `outpath` directory\n",
    "\n",
    "**Note:** number of frames for which we provide predictions is equal to `duration(seconds) * 1000 / stride(ms) / model_stride`, where stride is often 10ms for mfsc/mfcc features computation and model_stride in the example is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls vad-result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id, percent of voice frames, perplexity of the transcription\n",
    "!head vad-result/3.sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final transcription\n",
    "!head vad-result/3.tsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-frame transcription (# - is a blank token)\n",
    "!head vad-result/3.fwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-frame blank confidence\n",
    "!head vad-result/3.vad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result of VAD can be used to cut an audio into chunks for faster training/processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vad-result/3.fwt\", \"r\") as f:\n",
    "  plot_vad(\"audio/116-288045-0003.flac\", f.readline().strip())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
